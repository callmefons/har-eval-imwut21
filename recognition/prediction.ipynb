{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "print(\"Current working dir : %s\" % os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib # save/load models\n",
    "import pickle # save/load features\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn import preprocessing \n",
    "from sklearn.metrics import confusion_matrix # for confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels# for confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename_model):\n",
    "    return joblib.load(filename_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_objs(filename_objs):\n",
    "    with open(filename_objs, 'rb') as f:\n",
    "        try:\n",
    "            loaded_objs = pickle.load(f)\n",
    "            training_features = loaded_objs[0]\n",
    "            test_features = loaded_objs[1]\n",
    "            training_target  = loaded_objs[2]\n",
    "            test_target  = loaded_objs[3]\n",
    "            X_train = loaded_objs[4]\n",
    "            X_val = loaded_objs[5]\n",
    "            y_train = loaded_objs[6]\n",
    "            y_val = loaded_objs[7]\n",
    "            class_weight  = loaded_objs[8],\n",
    "            class_names  = loaded_objs[9]\n",
    "            n_classes = loaded_objs[10]\n",
    "        except:\n",
    "            print(\"Something went wrong when writing to the file\")\n",
    "\n",
    "    return (training_features, test_features, training_target, \n",
    "            test_target,X_train, X_val, y_train, y_val,\n",
    "            class_weight,class_names,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    preprocessing.LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_test, predictions):\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "    \n",
    "    f1_score_weighted = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    roc_auc_score = multiclass_roc_auc_score(y_test, predictions)\n",
    "    out = \"%.2f\" % (roc_auc_score * 100.0)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocauc_perclass(y_pred, y_test, class_names, n_classes):\n",
    "    # binarize to see roc_auc_score per class\n",
    "    y_pred_binarize = preprocessing.label_binarize(y_pred, classes=class_names)\n",
    "    y_test_binarize = preprocessing.label_binarize(y_test, classes=class_names)\n",
    "    \n",
    "    # compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    roc_auc_all = []\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_binarize[:, i], y_pred_binarize[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # compute micro-average ROC curve and ROC area\n",
    "    fpr[\"macro\"], tpr[\"macro\"], _ = roc_curve(y_test_binarize.ravel(), y_pred_binarize.ravel())\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    return (roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict data by each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create users bases on user_id in the database\n",
    "users = [i for i in range(112,185,1)] # 40 \n",
    "# users = [i for i in range(203,258,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "f1_test_all = []\n",
    "f1_val_all = []\n",
    "n_classes_all = []\n",
    "index = []\n",
    "i=0\n",
    "auc_test_all = []\n",
    "auc_val_all = []\n",
    "auc_class_all = []\n",
    "user_all = []\n",
    "\n",
    "for user in users:\n",
    "    \n",
    "    user_id = user\n",
    "    \n",
    "    filename_model = \"users-models/model_user_\"+str(user_id)+\".sav\"\n",
    "    filename_objs = \"users-models/model_user_\"+str(user_id)+\".pkl\"\n",
    "    \n",
    "    # load the model from disk\n",
    "    mcl = load_model(filename_model)\n",
    "    \n",
    "    # load the variables from disk\n",
    "    (training_features, test_features, training_target, \n",
    "     test_target, X_train, X_val, y_train, y_val,\n",
    "     class_weight,class_names,n_classes) = load_objs(filename_objs)\n",
    "    \n",
    "    y_pred_test = mcl.predict(test_features)\n",
    "    predictions_test = [round(value) for value in y_pred_test]\n",
    "    \n",
    "    y_pred_val = mcl.predict(X_val)\n",
    "    predictions_val = [round(value) for value in y_pred_val]\n",
    "    \n",
    "    roc_auc_test = rocauc_perclass(y_pred_test, test_target, class_names, n_classes)\n",
    "    roc_auc_val = rocauc_perclass(y_pred_val, y_val, class_names, n_classes)\n",
    "    \n",
    "    j=0\n",
    "    for name in class_names:\n",
    "        user_all.append(user_id)\n",
    "        auc_test_all.append(roc_auc_test[j])\n",
    "        auc_val_all.append(roc_auc_val[j])\n",
    "        n_classes_all.append(n_classes)\n",
    "        auc_class_all.append(name)\n",
    "        j+=1\n",
    "    \n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "d = {\n",
    "    'user_id':user_all,\n",
    "    'auc_test': auc_test_all, \n",
    "    'auc_val': auc_val_all,\n",
    "    'auc_class': auc_class_all,\n",
    "    'n_classes': n_classes_all,\n",
    "    }\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(data):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_full(df.sort_values(by=['f1_test'],ascending=False))\n",
    "df = df.dropna()\n",
    "df.describe()\n",
    "df.head()\n",
    "df = pd.DataFrame(df, columns=['user_id','auc_test','auc_val','auc_class','n_classes'])\n",
    "df.to_csv('data/accuracy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
